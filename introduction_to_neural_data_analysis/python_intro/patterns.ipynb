{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "remove-cell"
        ]
      },
      "outputs": [],
      "source": [
        "%config InlineBackend.figure_formats = ['svg']"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "z0y0eLBQuP_D"
      },
      "source": [
        "# Patterns, functions, and duck typing"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "BjSUlCU0uP_F"
      },
      "source": [
        "In the last section, we looked at the basic ingredients of Python (its data types) and the ways we can use them (iteration and logic, more formally known as \"control flow\").\n",
        "\n",
        "We also touched on the idea that both these ingredients and the way we mix them are designed so as to be consistent. This consistency, that lists behave like tuples behave like dicts (whenever that makes sense), is one of the hallmarks of Python. \n",
        "\n",
        "This consistency also gives rise to one of the most common patterns or idioms in Python: duck typing. Unlike languages that require you to strictly define what types of data a particular piece of code can work with, Python takes the standpoint that, \"If it walks like a duck and quacks like a duck, it's a duck.\" In this paradigm, it doesn't matter what kind of data you pass to a function so long as that data supports all the necessary operations.\n",
        "\n",
        "We'll explore some of this below."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "cZVD_P-cuP_G"
      },
      "source": [
        "## Patterns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pbKmshdyuP_G"
      },
      "source": [
        "Programmers are extremely lazy &mdash; a certain type of lazy. A good programmer is the kind of person who will spend six hours coding a solution to a problem that saves her six seconds thousands of times. \n",
        "\n",
        "As a result, since the 90s, programmers have collected and studied several dozen kinds of (good) solutions for how to _structure_ programs that come up over and over again. If you've written anything more complicated than a simple script, you might have even come up with one or two of these. Many of them only really apply to larger software projects. These solutions are known as [Design Patterns](https://www.oreilly.com/library/view/head-first-design/0596007124/), and they're the next step up the ladder in your programming education once you feel comfortable writing snippets of code."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xnLXQK_MuP_G"
      },
      "source": [
        "But let's make this concrete. To review, consider:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hKjvU9A9uP_G"
      },
      "outputs": [],
      "source": [
        "mylist = ['1', 1, 'pooh', 'piglet', [2, 7, True]]\n",
        "mytup = ('a', 1, 2, True)\n",
        "mystr = \"we can do this \\U0001F389\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wmysg7sGuP_H"
      },
      "source": [
        "As a trivial example of duck typing, note that we don't have to do anything special to print these variables: it's the same `print` function, regardless of the data type."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y1HXrGR_uP_H",
        "outputId": "efa9c9c5-6e3b-4090-f0a8-f121e099b96b"
      },
      "outputs": [],
      "source": [
        "print(mylist)\n",
        "print(mytup)\n",
        "print(mystr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lmG5ceCRuP_H"
      },
      "source": [
        "Less trivially, all of these are collections (they have a notion of \"contained in\") and they are iterable (we can get elements out of them one by one). Because of this, the code to iterate over all these collections and print elements one at a time is identical:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oMOS-33_uP_I",
        "outputId": "894433f9-8baa-4280-ebb5-1cfd616b3cca"
      },
      "outputs": [],
      "source": [
        "collection = mylist  # you can change this to different collections defined above\n",
        "\n",
        "for element in collection:\n",
        "    print(element)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lKRelwCtuP_I"
      },
      "source": [
        "This may seem simple, but it's because Python has taken one of the most common design patterns &mdash; the iterator pattern &mdash; and baked it right into the language."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "675fdlZ2uP_I"
      },
      "source": [
        "```{admonition} Exercise\n",
        "We've seen that functions like `print` and the iterator pattern are shared across data types. What are other examples of such similarities?\n",
        "```"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "_CMISqItuP_I"
      },
      "source": [
        "## Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zlTarPVtuP_J"
      },
      "source": [
        "Functions are the single greatest invention in the history of computer programming. By allowing us to reuse code, functions allow for greater abstraction (as we'll see), more readable code, and greater modularity (you don't need to to know the inner details of everything)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4SoaA6FtuP_J"
      },
      "source": [
        "More explicitly, functions are named blocks of code with inputs and a single output (though we can get around this restriction). To define a function, we can use the `def` keyword:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2OHHKeEZuP_J",
        "jupyter": {
          "outputs_hidden": false
        },
        "outputId": "dfb1d03d-4e13-4988-b984-7cce0bcac2f7"
      },
      "outputs": [],
      "source": [
        "def myfunc(x):\n",
        "    print(x + 1)\n",
        "    return 2 * x\n",
        "\n",
        "print(myfunc(4))\n",
        "y = myfunc(-7)\n",
        "print(y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jKEfDrAWuP_K"
      },
      "source": [
        "Here, `def` says we are about to define a function. This keyword is followed by the name of the function and a list of its *arguments* in parentheses. Python has several neat features in the way arguments are defined, including the ability to take arguments by name, to leave the number of arguments unspecified, and to give default values to certain arguments. \n",
        "\n",
        "Finally, the `return` keyword specifies the output of the function. Note that, like `for`, the line defining the function ends in a colon and the entire function body is indented."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AOr04IZPuP_K",
        "jupyter": {
          "outputs_hidden": false
        },
        "outputId": "42235122-cf24-4004-c190-19e66d8fa3f7"
      },
      "outputs": [],
      "source": [
        "def anotherfunc(x, y=2):  # y has the default value 2\n",
        "    z = x ** y  # x to the power y\n",
        "    return z / 2.0\n",
        "\n",
        "print(anotherfunc(4, 0.5))  # here we specify both x and y\n",
        "print(anotherfunc(4))  # here we specify only x, so y = 2"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "skEDKwDAuP_K"
      },
      "source": [
        "### Understanding variable scope \n",
        "\n",
        "Functions are like *black boxes*. The information that gets passed into them is bound to the input variable name, but this variable only exists while the function is running. This is a tricky topic that goes under the name of *variable scoping*, but the following examples illustrate that you have to be careful about what information is and isn't being passed into a function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Rh9kh3zuP_K",
        "jupyter": {
          "outputs_hidden": false
        },
        "outputId": "c22490c2-e13e-4405-846b-01ae4b32fc9d"
      },
      "outputs": [],
      "source": [
        "x = 'foo'\n",
        "print('x = ' + x)\n",
        "\n",
        "# appending an empty string seems like a sensible default\n",
        "def reverser(x, appender=''):  \n",
        "    \"\"\"\n",
        "    This is a docstring. It tells us what the function does. \n",
        "    This function reverses its input and appends its second argument to the end.\n",
        "    \"\"\"\n",
        "    print('x = ' + x)\n",
        "    return x[::-1] + appender\n",
        "\n",
        "print(help(reverser))\n",
        "\n",
        "print(reverser('bar'))\n",
        "print(reverser('elephant', ' monkey'))\n",
        "\n",
        "print('x = ' + x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jaLbsQHyuP_L"
      },
      "source": [
        "Note that the value of `x` inside the function had nothing to do with the value of `x` outside the function. Within the function, `x` took on the value of whatever we passed in as the first argument of reverser. When the function returned, x was restored to its original value.\n",
        "\n",
        "This may seem confusing, but we actually want this behavior. The fact that variables defined within the function live and die inside the function means that we can use functions without worrying that they will overwrite variables we ourselves define. Imagine if you used a function that had an argument `x` or defined a variable `data`. You may well have these variables running around in your own code, and scoping makes sure that you don't need to worry about someone else's function overwriting them.\n",
        "\n",
        "Functions take inputs, perform their work, and return outputs. You don't have to know what they are doing under the hood, and your own functions should play nice in the same way."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HwBLIMuruP_L"
      },
      "source": [
        "### What makes a good function?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZtHWlc0uP_L"
      },
      "source": [
        "Some things to consider:\n",
        "- functions do *one thing*\n",
        "- functions give us the chance to replace confusing behavior with clearly named behavior\n",
        "- functions allow us to obey the DRY principle (don't repeat yourself)\n",
        "- functions can call other functions\n",
        "\n",
        "So how about we practice by writing some code?"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "EVyiwhm3uP_L"
      },
      "source": [
        "## Handling multiple data sets:"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To illustrate these features, we'll be examining data from a comparative study of cognitive ability between macaques and lemurs [link](https://www.researchgate.net/publication/257075016_Lemurs_and_macaques_show_similar_numerical_sensitivity). Data are available [here](http://www.duke.edu/~jmp33/dibs/primates.tar.gz), but to make things easy, we will create a directory here on Colab and download the files directly from Google drive. After that, we will assume that these data live in a directory ```data/primates``` inside the working directory.\n",
        "\n",
        "To accomplish that, we'll use the `os` library:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e8Gp8H8zwwTQ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "if not os.path.isdir('data/primates'):\n",
        "  os.makedirs('data/primates')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then we'll use the `gdown` package to download the files directly from Google Drive:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import gdown\n",
        "data_urls = [\n",
        "    'https://drive.google.com/uc?export=download&id=1t-USz89jo4DPuyxUcr6DzX1FKdEeIOl6',\n",
        "    'https://drive.google.com/uc?export=download&id=1sZNoxO8Hf4sw6-dvtOKz2TYAydVnihHo',\n",
        "    'https://drive.google.com/uc?export=download&id=1lnl5zGDSWHZLKuiiKgPmwPuKBGGIv5AF',\n",
        "    'https://drive.google.com/uc?export=download&id=1eBYozAifLLbBfMMdU5CNfYTGuDzH4YnO',\n",
        "    'https://drive.google.com/uc?export=download&id=1QKRi0HdKoRDAUOdQJ05ozh065KN0D-IN'\n",
        "]\n",
        "data_names = [\n",
        "    'macaque.csv',\n",
        "    'trained_Macaque.csv',\n",
        "    'Black.csv',\n",
        "    'Mongoose.csv',\n",
        "    'Catta.csv'\n",
        "]\n",
        "prefix = 'data/primates/'\n",
        "for url, name in zip(data_urls, data_names):\n",
        "    gdown.download(url, prefix + name, quiet=True) \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AsNSvBO4uP_M"
      },
      "source": [
        "Normally, we could just use the `%ls` magic to get the list of files in a given directory:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "js83VYtFuP_M",
        "jupyter": {
          "outputs_hidden": false
        },
        "outputId": "6dc6ad83-dd04-4495-cae0-ec53f0d2b3d4"
      },
      "outputs": [],
      "source": [
        "%ls data/primates"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ZPkzIV2muP_M"
      },
      "source": [
        "But if we want to eventually moved over to pure Python, we will use the `os` library, which gives us operating system commands."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D-0t961xuP_M",
        "jupyter": {
          "outputs_hidden": false
        },
        "outputId": "e1772f42-ce1b-412b-ad15-bb7833993727"
      },
      "outputs": [],
      "source": [
        "pathparts = ('data', 'primates')\n",
        "\n",
        "# this command will work on both Windows and Mac/Unix\n",
        "# the * expands the tuple, so it's as if we'd written \n",
        "# os.path.join('data', 'basic_python', 'primates')\n",
        "fullpath = os.path.join(*pathparts)\n",
        "\n",
        "print(fullpath)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ySiLOleJuP_N",
        "jupyter": {
          "outputs_hidden": false
        },
        "outputId": "bac760dd-d958-414b-e66b-338a511c01d3"
      },
      "outputs": [],
      "source": [
        "datfiles = os.listdir(fullpath) # note: we're not guaranteed an order here\n",
        "print(datfiles)  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cxZC0SBnuP_N"
      },
      "source": [
        "Our first order of business is to figure out our analysis from a single dataset. "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Hzs0hLF2uP_N"
      },
      "source": [
        "```{warning}\n",
        "The analysis below uses DataFrames. We'll do more on those later. For now, just see what's possible and play along. The important thing is how we'll reorganize (or \"refactor\") the code below.\n",
        "```\n",
        "\n",
        "We'll load the `csv` file (which you can view as a spreadsheet) as a Pandas DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 319
        },
        "id": "-8U5nQ7zuP_N",
        "jupyter": {
          "outputs_hidden": false
        },
        "outputId": "febaf067-bfe4-4bb7-9b70-c6b60aca72da"
      },
      "outputs": [],
      "source": [
        "# make the filename by joining with the path (works cross-platform!)\n",
        "fname = os.path.join(fullpath, datfiles[0])  \n",
        "\n",
        "import pandas as pd\n",
        "# df is short for dataframe\n",
        "# in code with a lot of dataframes, we would choose a more descriptive name\n",
        "# index_col=0 says the first column of the file is row names \n",
        "df = pd.read_csv(fname, index_col=0)  \n",
        "\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ChndHmNbuP_O"
      },
      "source": [
        "We can find out some interesting things:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i8JRRh2SuP_O",
        "jupyter": {
          "outputs_hidden": false
        },
        "outputId": "3327ce36-e818-40da-9c38-a86cbd48d499"
      },
      "outputs": [],
      "source": [
        "df['Sub'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DBbenWLjuP_O",
        "jupyter": {
          "outputs_hidden": false
        },
        "outputId": "5282257f-99ea-468d-e2f6-c4b2c885356b"
      },
      "outputs": [],
      "source": [
        "df['Species'].unique(), df['Date'].unique()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LBaS0U-CuP_O"
      },
      "source": [
        "## Groupby: Split-Apply-Combine:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L5qRQrniuP_O"
      },
      "source": [
        "It's pretty typical in a dataset like this that we want to do some analysis for each subset of the data, however that subset is defined. Pandas makes this very easy:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "eteBQCYOuP_O",
        "jupyter": {
          "outputs_hidden": false
        },
        "outputId": "4ba5f6da-b21a-4cc0-a89e-e4cc257516ce"
      },
      "outputs": [],
      "source": [
        "# reading left to right: \n",
        "# group the data by subject, \n",
        "# take the accuracy and response time columns,\n",
        "# compute the mean of each\n",
        "\n",
        "df.groupby('Sub')[['Accuracy', 'RT']].mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 328
        },
        "id": "8NXIHvqAuP_P",
        "jupyter": {
          "outputs_hidden": false
        },
        "outputId": "c56aa792-d5cd-4684-94cb-e2491d86d581"
      },
      "outputs": [],
      "source": [
        "df.groupby(['Sub', 'Surface Area'])[['Accuracy', 'RT']].mean()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "B3RUd6g0uP_P"
      },
      "source": [
        "`groupby` has much more sophisticated behavior than this (if you want to group by something other than a specific set of columns, you can supply your own criterion), which you can read about [here](http://pandas.pydata.org/pandas-docs/dev/groupby.html).\n",
        "\n",
        "In addition, we can plot things like reaction time distributions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 355
        },
        "id": "85IQeJTeuP_P",
        "jupyter": {
          "outputs_hidden": false
        },
        "outputId": "2bcf79ac-b8f8-42b3-8f25-51bb4bf9d236"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "df[['Sub', 'RT']].boxplot(by='Sub');"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "EKRy3iEfuP_P",
        "jupyter": {
          "outputs_hidden": false
        },
        "outputId": "6133680f-0fde-45b0-fe93-393ec23b4020"
      },
      "outputs": [],
      "source": [
        "df['RT'].hist(by=df['Sub'], bins=100);"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "vmgTOQ5duP_P"
      },
      "source": [
        "Pandas plotting is best for quick and dirty plots; if we want to do better, we need to dive more into Matplotlib or Seaborn. We'll see how to prettify our outputs later on.\n",
        "\n",
        "But we can plot all on the same axis if we simply tell Pandas which axis to plot into.\n",
        "\n",
        "So here's our strategy:\n",
        "- create an axis object to plot into (`gca` = get current axis)\n",
        "- split the RT portion of the dataframe into groups using `groupby`\n",
        "- iterate over these groups (the iterator gives us a name and a dataframe for each group\n",
        "- call plot on each dataframe, passing the name as the label and the axis we want to reuse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "vF-aXsCDuP_Q",
        "jupyter": {
          "outputs_hidden": false
        },
        "outputId": "7e82d410-8f65-46d3-df50-ee2e6c93c302"
      },
      "outputs": [],
      "source": [
        "ax = plt.figure().gca()\n",
        "\n",
        "# now we're going to use iteration!\n",
        "# the grouped dataframe is a collection of (key, dataframe) tuples\n",
        "\n",
        "for name, grp in df.groupby('Sub'):  \n",
        "    # plot repeatedly into the same axes\n",
        "    grp['RT'].plot(kind='density', ax=ax, label=name.capitalize());\n",
        "\n",
        "plt.legend();  # draw plot legend\n",
        "\n",
        "# adjust x limits of plot\n",
        "plt.xlim(-1, 10);  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iqT6XEHCuP_Q"
      },
      "source": [
        "So we've seen that we can do some neat things with this individual dataset. In fact, we'd like to do these analyses and *aggregate* across all datasets.\n",
        "\n",
        "Here's the plan:\n",
        "    - load each datset in turn\n",
        "    - get the average RT and Accuracy for each animal, store it in a dataframe\n",
        "    - plot the RT curve for each animal\n",
        "    - load the next dataset, repeat"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5x8ZBH11uP_Q"
      },
      "source": [
        "## Multiple datasets: pulling it together:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pVBKBunRuP_Q"
      },
      "source": [
        "Let's try to combine the above code into a single chunk of code. We'll iterate over data files and simply repeat the same code each time. (Note how we made a good decision in encoding the file name in a variable we can change instead of hard coding it.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 537
        },
        "id": "dGGmaElCuP_R",
        "jupyter": {
          "outputs_hidden": false
        },
        "outputId": "702b206b-f851-434c-baa1-3d73968eedb3"
      },
      "outputs": [],
      "source": [
        "# make an empty piece to hold each dataframe\n",
        "df_pieces = []\n",
        "\n",
        "ax = plt.figure().gca()  # make a figure and get its current axis object\n",
        "\n",
        "# iterate over datfiles\n",
        "for f in datfiles:\n",
        "    fname = os.path.join(fullpath, f)\n",
        "    \n",
        "    df = pd.read_csv(fname, index_col=0)\n",
        "\n",
        "    mean_data = df.groupby('Sub')[['Accuracy', 'RT']].mean()\n",
        "    \n",
        "    df_pieces.append(mean_data)\n",
        "    \n",
        "    for name, grp in df.groupby('Sub'):\n",
        "        grp['RT'].plot(kind='density', ax=ax, label=name.capitalize());\n",
        "        \n",
        "plt.xlim(0, 6)\n",
        "\n",
        "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.);\n",
        "\n",
        "combined_data = pd.concat(df_pieces)\n",
        "\n",
        "combined_data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EqvqvaXbuP_R"
      },
      "source": [
        "Note that we basically just copied over the code from before. (For the slick arguments to `legend` that put the box outside and to the right, see example [here](http://matplotlib.org/users/legend_guide.html)."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "iq9wb0Z0uP_R"
      },
      "source": [
        "## Building code that lasts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OL3UAJEUuP_R"
      },
      "source": [
        "The above chunk of code is pretty nifty. It works, it produces good output, it's something we can back and run in six months to produce that figure.\n",
        "\n",
        "But how well will you understand that code in six months? What if you need to change it? What if we'd like to reuse the code elsewhere. Typically, researchers use a few approaches:\n",
        "- generate plots interactively when needed; don't bother with a script\n",
        "- modify this script as needed to produce new output\n",
        "- cut and paste from this script when you need to do something similar\n",
        "\n",
        "The first of these is a terrible idea. The others less so, but they have disadvantages:\n",
        "- if you modify this script, you need to remember what you modified and where so that you can produce the original figure again\n",
        "- if you cut and paste, and you later improve the code or find a bug, you need to remember all the places you cut and pasted, correct the code, and re-run\n",
        "- if you cut and paste, your code will contain lots of repetition; it will be harder to see how what you're doing differs across scripts\n",
        "    \n",
        "The strategy that good coders use to surmount these difficulties is *code reuse*. There are lots of ways to reuse code, but the oldest and arguably best is to modularize our code by writing functions. Modular code is built up from smaller subunits. Originally, these units were scripts, but over time, the preferred method is to create functions. Functions are like scripts in that they are named sections of code, but they have a few advantages over scripts, as we will see."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Z6qo0FnPuP_R"
      },
      "source": [
        "### Modularization strategy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WR181GxBuP_S"
      },
      "source": [
        "For scientists, the path to modularization generally takes this form:\n",
        "- start by exploring data interactively in the console or a notebook\n",
        "- tidy up the code in a notebook that illustrates a particular analysis\n",
        "- when you start to see chunks of code that do a single (non-obvious) task, collect those chunks into functions\n",
        "- rewrite the analysis to call the functions\n",
        "- remove the functions from the notebook and put them into modules that can be imported\n",
        "\n",
        "The emphasis here is first on deciding what we want to do (exploring analyses), getting it working (illustrating in a notebook), and only lastly on making our code cleaner and more reusable. The same goes for making our code faster, which comes as a last step. As you become a better programmer, you will develop the ability to think about reuse and speed from the early stages, but even very good coders can be bad guessers at how best to design things early on."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IHBfiE2vuP_S",
        "jupyter": {
          "outputs_hidden": false
        }
      },
      "outputs": [],
      "source": [
        "def get_data_files(pathparts):\n",
        "    \"\"\"\n",
        "    This function takes an iterable of path parts (directories), \n",
        "    finds all files in that directory, and returns a list of those files.\n",
        "    \"\"\"\n",
        "    \n",
        "    import os\n",
        "    \n",
        "    fullpath = os.path.join(*pathparts)\n",
        "\n",
        "    datfiles = os.listdir(fullpath)\n",
        "    \n",
        "    # now add the fullpath to each of these file names so\n",
        "    # we output a list of absolute paths\n",
        "    \n",
        "    output_list = [os.path.join(fullpath, f) for f in datfiles]  # whoa! \n",
        "    \n",
        "    return output_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JdfVX0cKuP_S",
        "jupyter": {
          "outputs_hidden": false
        },
        "outputId": "15c8d7ba-b457-46d6-86c2-cf7c6b307ac3"
      },
      "outputs": [],
      "source": [
        "print(get_data_files(pathparts))  # should work as before\n",
        "\n",
        "print(get_data_files(list(pathparts)))  # even works if the input is a list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kA1m5dRyuP_S"
      },
      "source": [
        "Note that Python is smart enough to use a list, since the `*` operator will convert any iterable object (one that can be stepped through) into a tuple and then unpack as normal.\n",
        "\n",
        "Also, we used a fancy trick inside called a [list comprehension](https://docs.python.org/2/tutorial/datastructures.html#list-comprehensions) that makes it easy to do some operations where we would normally have to iterate (i.e., use a `for` loop).\n",
        "\n",
        "And we can define a couple of other functions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XtRm3aBsuP_S",
        "jupyter": {
          "outputs_hidden": false
        }
      },
      "outputs": [],
      "source": [
        "def extract_data(df):\n",
        "    \"\"\"\n",
        "    Calculate the mean RT and Accuracy per subject for the dataframe df. \n",
        "    Return result as a data frame.\n",
        "    \"\"\"\n",
        "    \n",
        "    groupvar = 'Sub'\n",
        "    colvars = ['Accuracy', 'RT']\n",
        "    \n",
        "    return df.groupby(groupvar)[colvars].mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tbqfj_eCuP_T",
        "jupyter": {
          "outputs_hidden": false
        }
      },
      "outputs": [],
      "source": [
        "def plot_RT_dist(df, ax):\n",
        "    \"\"\"\n",
        "    Given a file name and axis object, plot the RT distribution for \n",
        "    each animal in the file into the axis object.\n",
        "    \"\"\"\n",
        "    \n",
        "    groupvar = 'Sub'\n",
        "    colvar = 'RT'\n",
        "    \n",
        "    for name, grp in df.groupby(groupvar):\n",
        "        grp[colvar].plot(kind='density', ax=ax, label=name.capitalize());\n",
        "        \n",
        "    return ax"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJ1Hnnh6uP_T"
      },
      "source": [
        "Now let's use those functions to put together the entire analysis into a single function.\n",
        "\n",
        "Note how much easier this is to read than the code before. Even though it's more lines, calling functions with descriptive names like `plot_RT_dist` makes the goal we're trying to achieve clearer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "abSazeWLuP_T",
        "jupyter": {
          "outputs_hidden": false
        }
      },
      "outputs": [],
      "source": [
        "def do_all_analysis(files):\n",
        "    \"\"\"\n",
        "    This function plots the reaction time density for each subject in each file\n",
        "    contained in the iterable files. It also calculates the mean accuracy and \n",
        "    reaction time for each subject and returns these in a data frame.\n",
        "    Files should be full file paths.\n",
        "    \"\"\"\n",
        "    import matplotlib.pyplot as plt\n",
        "    import pandas as pd\n",
        "    \n",
        "    df_pieces = []\n",
        "    ax = plt.figure().gca()\n",
        "    \n",
        "    for f in files:\n",
        "        # read in data\n",
        "        df = pd.read_csv(f, index_col=0)\n",
        "        \n",
        "        # process summary data from df\n",
        "        summary_data = extract_data(df)\n",
        "        df_pieces.append(summary_data)\n",
        "        \n",
        "        # plot Reaction Time distribution\n",
        "        plot_RT_dist(df, ax)\n",
        "        \n",
        "    # add legend to figure\n",
        "    plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.);\n",
        "    \n",
        "    # get figure corresponding to axis\n",
        "    fig = ax.get_figure()  \n",
        "    \n",
        "    # concatenate all extracted dataframe pieces into one\n",
        "    combined_data = pd.concat(df_pieces)\n",
        "    \n",
        "    # now return a tuple with the combined data frame and the figure object\n",
        "    return combined_data, fig "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 971
        },
        "id": "R5VqxqWQuP_T",
        "jupyter": {
          "outputs_hidden": false
        },
        "outputId": "2a8acae5-4437-45dc-9f25-bb05f11b65d5"
      },
      "outputs": [],
      "source": [
        "flist = get_data_files(pathparts)\n",
        "\n",
        "summary_data, fig = do_all_analysis(flist)\n",
        "\n",
        "plt.xlim(0, 6);\n",
        "\n",
        "summary_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D1y3GPn9uP_U"
      },
      "source": [
        "# Writing your own modules:\n",
        "\n",
        "First, let's download the module:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a-PEYxCi5LrR",
        "outputId": "4bfb517a-9e9e-4619-d2d3-0d9d2fefcb5a"
      },
      "outputs": [],
      "source": [
        "!gdown 1keZWN340R0KT3vxF9ul-2xb4avYRR31v --quiet"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```{tip}\n",
        "This is an alternate way to run gdown for downloading files from Google Drive. It uses the same `gdown` package we imported above, but the `!` at the beginning of the line tells Jupyter to run this at the command line rather than in Python.\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dkWPwfFOuP_U"
      },
      "source": [
        "In `lemurs.py` (the file we just downloaded), I've extracted this code into its own standalone module. That is, I've excerpted the functions into their own file. There are at least three ways we can use this separate code file:\n",
        "\n",
        "1. Because the file has a line\n",
        "\n",
        "    ~~~python\n",
        "    if __name__ == '__main__':\n",
        "    ~~~\n",
        "    \n",
        "    the code will check to see if it is being run as a standalone module from the command line. In that case, the special variable `__name__` has the value `'__main__'`, and the code following the colon will execute. In that case, we can simply type\n",
        "    \n",
        "    ~~~\n",
        "    python lemurs.py\n",
        "    ~~~\n",
        "    \n",
        "    at the command line to load the module and run all the code that follows the `if` statement above.\n",
        "\n",
        "1. Similarly, we can use the `%run` magic function in the IPython notebook:\n",
        "\n",
        "    ~~~\n",
        "    %run lemurs\n",
        "    ~~~\n",
        "    \n",
        "    This will have the same effect, except we will be able to carry on at the end of the code with all the variables still intact. This is great if we want to pull part of our analysis into a separate file but try out new ideas in the notebook from the place the code left off.\n",
        "    \n",
        "1. Finally, if we just want to make use of some function in the module we can do\n",
        "    \n",
        "    ~~~python\n",
        "    import lemurs\n",
        "    ~~~\n",
        "    \n",
        "    which loads all of the definitions from the module into memory where we can call them. This is *exactly* what we did in importing pandas or matplotlib, but this time with our own code!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 319
        },
        "id": "TgoH1MuyuP_U",
        "jupyter": {
          "outputs_hidden": false
        },
        "outputId": "5cab9279-22cb-412b-b474-3111572d5dc1"
      },
      "outputs": [],
      "source": [
        "%run lemurs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HbpN2fZpuP_U",
        "jupyter": {
          "outputs_hidden": false
        },
        "outputId": "26633cd2-b950-4659-c803-2553a38b1e20"
      },
      "outputs": [],
      "source": [
        "import lemurs\n",
        "\n",
        "dir(lemurs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I_7w9tUcuP_V",
        "jupyter": {
          "outputs_hidden": false
        },
        "outputId": "94de04b7-983e-4088-c88a-9c3da3f34dca"
      },
      "outputs": [],
      "source": [
        "lemurs.get_data_files(pathparts)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As we've seen, Python (and its ecosystem) provide a powerful set of tools for moving quickly from prototyping in the notebook to building modular programs that automate data analysis. As projects get bigger and their corresponding pipelines more complex, this workflow is essential to produce maintainable code and reproducible results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "colab": {
      "collapsed_sections": [
        "Hzs0hLF2uP_N"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "vscode": {
      "interpreter": {
        "hash": "f9e02e0b94c2bdff7abf49239ef598c5e519e47cb46f883733d825e84045e64b"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
