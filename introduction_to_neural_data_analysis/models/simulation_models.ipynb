{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting simulated models\n",
    "In our last class, we looked at fitting models based on a known probability distribution. This let us write down a likelihood function that we could calculate and subsequently optimize to find the model's parameters. But what if the probability distribution changes from trial-to-trial? What about models where there's no simple formula we can write down?\n",
    "\n",
    "Thankfully, we can *still* make progress in cases like this because all that is required is that we can *compute* the likelihood algorithmically, not necessarily that it has a simple form we can write down. For this exercise, we'll be examining a very simple example of this by fitting a reinforcement learning model to data from a two-alternative forced choice (2AFT) task.\n",
    "\n",
    "## The task\n",
    "The task is simple: subjects are repeatedly presented with a pair of choices. One choice (A), delivers a reward of 2 units 70% of the time (and 0 otherwise), while another option (B) delivers a reward of 4 units 40% of the time. (**Quick**: which is better?) Subjects learn by trial and error about the values of each option and attempt to maximize reward.\n",
    "\n",
    "The [data set](https://drive.google.com/file/d/12h6xHzDIATnbf2pqApKNA9pBCAqdkHnq/view?usp=sharing) for this session consists of two vectors, one entry per trial, of a single subject's choices and outcomes.\n",
    "\n",
    "\n",
    "````{admonition} Exercise\n",
    "1. Load the data. Which option does the subject choose more frequently over time? You can download the data using\n",
    "\n",
    "```\n",
    "!gdown 12h6xHzDIATnbf2pqApKNA9pBCAqdkHnq\n",
    "```\n",
    "````\n",
    "\n",
    "### Solution:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "!gdown 12h6xHzDIATnbf2pqApKNA9pBCAqdkHnq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.io as sci\n",
    "\n",
    "# Load data\n",
    "vars = sci.loadmat('choice.mat')\n",
    "\n",
    "# check choice frequency\n",
    "choice= vars['choice']\n",
    "outcome = vars['outcome']\n",
    "n1 = np.sum(choice == 1)\n",
    "n2 = np.sum(choice == 2)\n",
    "print(f\"choice 1: {n1}, choice 2: {n2}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The model\n",
    "Our goal is to fit a simple reinforcement learning algorithm to these data. This model consists of two pieces:\n",
    "\n",
    "1. A *value model* that describes the benefit of choosing each option, along with a method of updating these values based on outcomes.\n",
    "1. A *choice model* or *policy* that uses an estimate of the value of each option to make decisions.\n",
    "\n",
    "The value model will assign each option a value $Q$, its best estimate of the average reward gained by choosing that option. If, on a particular trial, choosing option A results in reward $R$, then we update the value of $Q_A$ according to\n",
    "\n",
    "$$\n",
    "Q_A \\leftarrow Q_A + \\alpha (R - Q_A)\n",
    "$$\n",
    "\n",
    "with $\\alpha\\in [0, 1]$ the learning rate and $R - Q_A$ the difference in observed and expected outcomes &mdash; the *reward prediction error*. Option B is updated in the same way when it is chosen.\n",
    "\n",
    "When making choices, we assume the agent uses a *softmax* (in this case logistic) function that transforms the difference in value between options A and B into a probability. That is\n",
    "\n",
    "$$\n",
    "p(B) = \\frac{1}{1 + e^{-\\beta (Q_B - Q_A)}}\n",
    "$$\n",
    "\n",
    "and $p(A) = 1 - p(B)$. The parameter $\\beta$ captures the variability in choice, with large $\\beta$ corresponding to an agent that always chooses the option with higher $Q$ and $\\beta = 0$ an agent that chooses randomly.\n",
    "\n",
    "Our goal will be to use the observed choices and outcomes to find the most likely values of the agent's parameters $\\alpha$ and $\\beta$. To do so, we will maximize the likelihood of the observed choices by adjusting the parameter values.\n",
    "\n",
    "```{admonition} Exercise\n",
    "1. Write a function that takes as inputs a vector of parameters, a vector of choices, and a vector of outcomes and returns the log likelihood of the choices and outcomes given the parameters. You will want to break this down into steps:\n",
    "    1. The strategy is to loop over trials. For each trial you want to:\n",
    "        1. Calculate the probabilities of choosing A or B given $Q_A$ and $Q_B$.\n",
    "        1. Use these probabilities to compute the log likelihood of the **observed choice**. (**Hint:** if one has two mutually exclusive options, each chosen with a given probability, what's the distribution? What form does the likelihood take?)\n",
    "        1. Calculate the reward prediction error (RPE).\n",
    "        1. Use the RPE to update the $Q$-values **for the next trial**.\n",
    "    1. The result will be a vector of log likelihoods, one per trial. If the trials are all independent (given the $Q$s), then there is a straightforward way of combining all these log likelihoods into a single value to be returned by your function.\n",
    "1. Write a function to be optimized by SciPy. Two important notes:\n",
    "    1. Your function to be optimized can only take a single input, the vector of parameters.\n",
    "    1. SciPy only finds the **minimum** of functions, whereas we'd like to **maximize** the log likelihood.\n",
    "1. Optimize your function to find the maximum likelihood values of $\\alpha$ and $\\beta$. Since we need $\\alpha \\in [0, 1]$ and $\\beta \\ge 0$, you'll need to give the optimizer constraints (using the `bounds=` [keyword argument to `minimize`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html).) If a particular parameter is only bounded on one side (e.g., below) or unbounded, you can use `(0, None)` or `(None, None)`, respectively.\n",
    "```\n",
    "\n",
    "### Solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "def LL_RL(beta, choice, outcome):\n",
    "    \"\"\"\n",
    "    Given model parameters in the vector beta, calculate the log \n",
    "    likelihood of observed data given choices and outcomes.\n",
    "    \"\"\"\n",
    "    Ntrials, temp = choice.shape\n",
    "    alpha = beta[1]  # learning rate\n",
    "    gamma = beta[0]  # typically called beta\n",
    "\n",
    "    # allocate empty arrays for Q values and log likelihoods\n",
    "    Q = np.empty((Ntrials, 2))\n",
    "    LL = np.empty((Ntrials, 1))\n",
    "    \n",
    "    # initialize action values to 0\n",
    "    Q[0, :] = 0\n",
    "    \n",
    "    # loop over trials\n",
    "    for ii in range(Ntrials):\n",
    "        QA = Q[ii, 0]\n",
    "        QB = Q[ii, 1]\n",
    "\n",
    "        # probability of choosing B\n",
    "        pB = 1 / (1 + np.exp(-gamma * (QB - QA)))\n",
    "        \n",
    "        # calculate log likelihood based on observed choice\n",
    "        if choice[ii][0] == 1:\n",
    "            LL[ii] = np.log(1 - pB)\n",
    "        else:\n",
    "            LL[ii] = np.log(pB)\n",
    "                \n",
    "        # update action value for this choice according to\n",
    "        # learning rule\n",
    "        this_Q = Q[ii, choice[ii][0] - 1]\n",
    "        RPE = outcome[ii][0] - this_Q\n",
    "        \n",
    "        new_Q = this_Q + alpha * RPE\n",
    "        \n",
    "        if ii < Ntrials - 1:\n",
    "            Q[ii + 1, :] = Q[ii, :][0]\n",
    "            Q[ii + 1, choice[ii][0] - 1] = new_Q\n",
    "        \n",
    "    return LL\n",
    "\n",
    "# function we want to **minimize**\n",
    "# when trials are independent, total log likelihood is\n",
    "# just a sum of log likelihoods for each trial\n",
    "def cost(beta, choice, outcome):\n",
    "    return -np.sum(LL_RL(beta, choice, outcome))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "# fit model\n",
    "bnds = ((0.001, np.inf), (0, 1))\n",
    "res = minimize(cost, x0 = [1, 0.5], args = (choice, outcome), bounds = bnds)\n",
    "\n",
    "print(f\"params = {res.x}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.remove('choice.mat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "book",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a97c0be00662db3bed8fe12bace3dd68dcba4f9aa04406cb661d7504ba5e85ce"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
